{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torchvision\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence\nimport torchvision.transforms as transforms\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\n\n\nfrom tqdm import tqdm\nimport os\nimport gc\nimport pandas as pd\nimport itertools\nfrom tqdm.autonotebook import tqdm\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\n\nfrom collections import Counter\nimport nltk\nnltk.download('punkt')\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:29:29.431518Z","iopub.execute_input":"2023-01-03T20:29:29.431847Z","iopub.status.idle":"2023-01-03T20:29:33.344720Z","shell.execute_reply.started":"2023-01-03T20:29:29.431764Z","shell.execute_reply":"2023-01-03T20:29:33.343703Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"INPUT_IMAGES_DIR = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/\"\nLABEL_PATH = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\nOUTPUT_PATH = \"/kaggle/working\"","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:03.232704Z","iopub.execute_input":"2023-01-03T20:30:03.233894Z","iopub.status.idle":"2023-01-03T20:30:03.240311Z","shell.execute_reply.started":"2023-01-03T20:30:03.233852Z","shell.execute_reply":"2023-01-03T20:30:03.239194Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(LABEL_PATH, delimiter=\"|\")\ndf.columns = ['image', 'caption_number', 'caption']\ndf['caption'] = df['caption'].str.lstrip()\ndf['caption_number'] = df['caption_number'].str.lstrip()\ndf.loc[19999, 'caption_number'] = \"4\"\ndf.loc[19999, 'caption'] = \"A dog runs across the grass .\"\nids = [id_ for id_ in range(len(df) // 5) for i in range(5)]\ndf['id'] = ids\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:03.728002Z","iopub.execute_input":"2023-01-03T20:30:03.728346Z","iopub.status.idle":"2023-01-03T20:30:04.383451Z","shell.execute_reply.started":"2023-01-03T20:30:03.728317Z","shell.execute_reply":"2023-01-03T20:30:04.382516Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"            image caption_number  \\\n0  1000092795.jpg              0   \n1  1000092795.jpg              1   \n2  1000092795.jpg              2   \n3  1000092795.jpg              3   \n4  1000092795.jpg              4   \n\n                                             caption  id  \n0  Two young guys with shaggy hair look at their ...   0  \n1  Two young , White males are outside near many ...   0  \n2   Two men in green shirts are standing in a yard .   0  \n3       A man in a blue shirt standing in a garden .   0  \n4            Two friends enjoy time spent together .   0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption_number</th>\n      <th>caption</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000092795.jpg</td>\n      <td>0</td>\n      <td>Two young guys with shaggy hair look at their ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000092795.jpg</td>\n      <td>1</td>\n      <td>Two young , White males are outside near many ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000092795.jpg</td>\n      <td>2</td>\n      <td>Two men in green shirts are standing in a yard .</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000092795.jpg</td>\n      <td>3</td>\n      <td>A man in a blue shirt standing in a garden .</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000092795.jpg</td>\n      <td>4</td>\n      <td>Two friends enjoy time spent together .</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df[\"length\"] = df[\"caption\"].apply(lambda row: len(row.strip().split()))","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:04.385371Z","iopub.execute_input":"2023-01-03T20:30:04.385750Z","iopub.status.idle":"2023-01-03T20:30:04.556032Z","shell.execute_reply.started":"2023-01-03T20:30:04.385715Z","shell.execute_reply":"2023-01-03T20:30:04.555135Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom collections import Counter\n\nclass Vocabulary():\n    def __init__(self, df,vocab_threshold,vocab_file='vocab.pkl',\n               start_word=\"<start>\",end_word=\"<end>\",unk_word=\"<unk>\",vocab_from_file=False):\n        self.vocab_threshold = vocab_threshold\n        self.vocab_file = vocab_file\n        self.start_word = start_word\n        self.end_word = end_word\n        self.unk_word = unk_word\n        self.vocab_from_file = vocab_from_file\n        self.df = df\n        self.get_vocab()\n        \n    def get_vocab(self):\n        \"\"\"Load the vocabulary from file OR build the vocabulary from scratch.\"\"\"\n        if os.path.exists(self.vocab_file) & self.vocab_from_file:\n            with open(self.vocab_file, 'rb') as f:\n                vocab = pickle.load(f)\n                self.word2idx = vocab.word2idx\n                self.idx2word = vocab.idx2word\n            print('Vocabulary successfully loaded from vocab.pkl file!')\n        else:\n            self.build_vocab()\n            with open(self.vocab_file, 'wb') as f:\n                pickle.dump(self, f)\n                \n    def build_vocab(self):\n        \"\"\"Populate the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n        self.init_vocab()\n        self.add_word(self.start_word)\n        self.add_word(self.end_word)\n        self.add_word(self.unk_word)\n        self.add_captions()\n\n    def init_vocab(self):\n        \"\"\"Initialize the dictionaries for converting tokens to integers (and vice versa).\"\"\"\n        self.word2idx = {}\n        self.idx2word = {}\n        self.idx = 0 \n\n    def add_word(self, word):\n        \"\"\"Add a token to the vocabulary.\"\"\"\n        if not word in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n\n    def add_captions(self):\n        \"\"\"Loop over training captions and add all tokens to the vocabulary that meet or exceed the threshold.\"\"\"\n        counter = Counter()\n        x = df['caption'].apply(lambda row:counter.update(nltk.tokenize.word_tokenize(row)))\n        words = [word for word, cnt in counter.items() if cnt>=self.vocab_threshold]\n        for i, word in enumerate(words):\n            self.add_word(word)\n\n    def __call__(self, word):\n        if not word in self.word2idx:\n            return self.word2idx[self.unk_word]\n        return self.word2idx[word]\n    \n    def __len__(self):\n        return len(self.word2idx)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:05.032418Z","iopub.execute_input":"2023-01-03T20:30:05.032777Z","iopub.status.idle":"2023-01-03T20:30:05.046415Z","shell.execute_reply.started":"2023-01-03T20:30:05.032748Z","shell.execute_reply":"2023-01-03T20:30:05.044934Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def collate_batch(batch):\n    image_list,caption_list,caplen_list = [],[],[]\n    for (image, caption,caplen) in batch:\n        image_list.append(image)\n        caption_list.append(caption)\n        caplen_list.append(caplen)\n    \n    image_list = torch.stack(image_list)\n    caplen_list = torch.tensor(caplen_list)\n    caption_list = pad_sequence(caption_list, batch_first=True, padding_value=0)\n    return image_list,caption_list,caplen_list.unsqueeze(1)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:05.852149Z","iopub.execute_input":"2023-01-03T20:30:05.852520Z","iopub.status.idle":"2023-01-03T20:30:05.861156Z","shell.execute_reply.started":"2023-01-03T20:30:05.852478Z","shell.execute_reply":"2023-01-03T20:30:05.858057Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class ImageCaptioningDataset(torch.utils.data.Dataset):\n    def __init__(self,df,transform,mode,batch_size,vocab_threshold):\n        self.caption_lengths = df['length']\n        self.batch_size = batch_size\n        self.df = df\n        self.transform = transform\n        self.mode = mode\n        if mode == 'train':\n            self.vocab = Vocabulary(df,vocab_threshold,'vocab.pkl1',vocab_from_file=False)\n        elif (mode=='val') or (mode=='test'):\n            self.vocab = Vocabulary(df,vocab_threshold,'vocab_pkl1',vocab_from_file=True)\n    def __getitem__(self,index):\n        if (self.mode == 'train') or (self.mode =='val'):\n            image = Image.open(f\"{INPUT_IMAGES_DIR}/{self.df['image'][index]}\")\n            image = self.transform(image)\n\n            #Convert caption to tensor of word ids \n            tokens = nltk.tokenize.word_tokenize(self.df['caption'][index].lower())\n            caption = []\n            caption.append(self.vocab(self.vocab.start_word))\n            caption.extend([self.vocab(token) for token in tokens])\n            caption.append(self.vocab(self.vocab.end_word))\n            caption = torch.Tensor(caption).long()\n            \n            caplen = self.df['length'][index]\n\n            return (image,caption,caplen)\n        else:\n            p_image = Image.open(f\"{INPUT_IMAGES_DIR}/{self.df['image'][index]}\").convert('RGB')\n            image = np.array(p_image)\n            trans_image = self.transform(p_image)\n\n            return image, trans_image,self.df['caption'][index]\n\n    def get_train_indices(self):\n        sel_length = np.random.choice(self.caption_lengths)\n        all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]\n        indices = list(np.random.choice(all_indices, size=self.batch_size))\n        return indices\n\n    def __len__(self):\n        return len(self.df['caption'])","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:06.462333Z","iopub.execute_input":"2023-01-03T20:30:06.463405Z","iopub.status.idle":"2023-01-03T20:30:06.476344Z","shell.execute_reply.started":"2023-01-03T20:30:06.463362Z","shell.execute_reply":"2023-01-03T20:30:06.475035Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"mode ='train'\n## TODO #1: Select appropriate values for the Python variables below.\nbatch_size = 128         # batch size\nvocab_threshold = 6        # minimum word count threshold\nembed_size = 512           # dimensionality of image and word embeddings\nhidden_size = 512          # number of features in hidden state of the RNN decoder\nnum_epochs = 2             # number of training epochs (1 for testing)\nsave_every = 1             # determines frequency of saving model weights\nprint_every = 200          # determines window for printing average loss\nlog_file = 'training_log.txt'       # name of file with saved training loss and perplexity","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:07.052092Z","iopub.execute_input":"2023-01-03T20:30:07.052440Z","iopub.status.idle":"2023-01-03T20:30:07.058694Z","shell.execute_reply.started":"2023-01-03T20:30:07.052411Z","shell.execute_reply":"2023-01-03T20:30:07.057403Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"transform_train = transforms.Compose([ \n    transforms.Resize((256,256)),                          # smaller edge of image resized to 256                      # get 224x224 crop from random location\n    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n    ])\ntransform_test = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(), \n    ])","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:07.552149Z","iopub.execute_input":"2023-01-03T20:30:07.552523Z","iopub.status.idle":"2023-01-03T20:30:07.558633Z","shell.execute_reply.started":"2023-01-03T20:30:07.552482Z","shell.execute_reply":"2023-01-03T20:30:07.557550Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train, test =train_test_split(df,test_size=0.2,shuffle=True)\ntrain_df,valid_df = train_test_split(train,test_size=0.2,shuffle=True)\ntrain_df, valid_df, test = train_df.reset_index(drop=True), valid_df.reset_index(drop=True), test.reset_index(drop=True)\n\ntrain_dataset = ImageCaptioningDataset(train_df,transform_train,mode,batch_size,vocab_threshold)\nvalid_dataset = ImageCaptioningDataset(valid_df,transform_train,'val',batch_size,vocab_threshold)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True,\n                                               collate_fn=collate_batch,num_workers=2)\nvalid_dataloader = torch.utils.data.DataLoader(valid_dataset,batch_size=batch_size,shuffle=True,\n                                               collate_fn=collate_batch,num_workers=2)\n# indices = train_dataset.get_train_indices()\n# initial_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices=indices)\n#train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_sampler=torch.utils.data.sampler.BatchSampler(sampler=initial_sampler,batch_size=batch_size,drop_last=False))","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:08.208313Z","iopub.execute_input":"2023-01-03T20:30:08.209408Z","iopub.status.idle":"2023-01-03T20:30:43.972652Z","shell.execute_reply.started":"2023-01-03T20:30:08.209362Z","shell.execute_reply":"2023-01-03T20:30:43.971705Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"test_dataset = ImageCaptioningDataset(test,transform_test,'test',1,0)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=1,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:43.974415Z","iopub.execute_input":"2023-01-03T20:30:43.974801Z","iopub.status.idle":"2023-01-03T20:30:43.999128Z","shell.execute_reply.started":"2023-01-03T20:30:43.974767Z","shell.execute_reply":"2023-01-03T20:30:43.998198Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Vocabulary successfully loaded from vocab.pkl file!\n","output_type":"stream"}]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \"\"\"\n    Encoder.\n    shift to only output the feature map\n    \"\"\"\n\n    def __init__(self, encoded_image_size=14):\n        super(Encoder, self).__init__()\n        self.enc_image_size = encoded_image_size\n\n        resnet = torchvision.models.resnet50(pretrained=True)  # pretrained ImageNet ResNet-101\n\n        # Remove linear and pool layers (since we're not doing classification)\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n\n        # Resize image to fixed size to allow input images of variable size\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n\n        self.fine_tune()\n\n    def forward(self, images):\n        \"\"\"\n        Forward propagation.\n        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n        :return: encoded images\n        \"\"\"\n        feature_map = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n        #out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n        #out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n        return feature_map\n\n    def fine_tune(self, fine_tune=False):\n        \"\"\"\n        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n        :param fine_tune: Allow?\n        \"\"\"\n        for p in self.resnet.parameters():\n            p.requires_grad = False\n        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n        for c in list(self.resnet.children())[5:]:\n            for p in c.parameters():\n                p.requires_grad = fine_tune","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:31:44.789256Z","iopub.execute_input":"2023-01-03T20:31:44.789649Z","iopub.status.idle":"2023-01-03T20:31:44.798679Z","shell.execute_reply.started":"2023-01-03T20:31:44.789612Z","shell.execute_reply":"2023-01-03T20:31:44.797719Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class Spatial_attention(nn.Module):\n    \"\"\"\n    Attention Network.\n    \"\"\"\n\n    def __init__(self,feature_map,decoder_dim,K = 512):\n        \"\"\"\n        :param feature_map: feature map in level L\n        :param decoder_dim: size of decoder's RNN\n        \"\"\"\n        super(Spatial_attention, self).__init__()\n        _,C,H,W = tuple([int(x) for x in feature_map])\n        self.W_s = nn.Parameter(torch.randn(C,K))\n        self.W_hs = nn.Parameter(torch.randn(K,decoder_dim))\n        self.W_i = nn.Parameter(torch.randn(K,1))\n        self.bs = nn.Parameter(torch.randn(K))\n        self.bi = nn.Parameter(torch.randn(1))\n        self.tanh = nn.Tanh()\n        self.softmax = nn.Softmax(dim = 0)  # softmax layer to calculate weights\n        \n    def forward(self, feature_map, decoder_hidden):\n        \"\"\"\n        Forward propagation.\n        :param feature_map: feature map in level L(batch_size, C, H, W)\n        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n        :return: alpha\n        \"\"\"\n        V_map = feature_map.view(feature_map.shape[0],2048,-1) \n        V_map = V_map.permute(0,2,1)#(batch_size,W*H,C)\n        # print(V_map.shape)\n        # print(\"m1\",torch.matmul(V_map,self.W_s).shape)\n        # print(\"m2\",torch.matmul(decoder_hidden,self.W_hs).shape)\n        att = self.tanh((torch.matmul(V_map,self.W_s)+self.bs) + (torch.matmul(decoder_hidden,self.W_hs).unsqueeze(1)))#(batch_size,W*H,C)\n        # print(\"att\",att.shape)\n        alpha = self.softmax(torch.matmul(att,self.W_i) + self.bi)\n#         print(\"alpha\",alpha.shape)\n        alpha = alpha.squeeze(2)\n        feature_map = feature_map.view(feature_map.shape[0],2048,-1) \n        # print(\"feature_map\",feature_map.shape)\n        # print(\"alpha\",alpha.shape)\n        temp_alpha = alpha.unsqueeze(1)\n        attention_weighted_encoding = torch.mul(feature_map,temp_alpha)\n        return attention_weighted_encoding,alpha","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:51.304942Z","iopub.execute_input":"2023-01-03T20:30:51.305293Z","iopub.status.idle":"2023-01-03T20:30:51.317098Z","shell.execute_reply.started":"2023-01-03T20:30:51.305265Z","shell.execute_reply":"2023-01-03T20:30:51.315944Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Channel_wise_attention(nn.Module):\n    \"\"\"\n    Attention Network.\n    \"\"\"\n\n    def __init__(self,feature_map,decoder_dim,K = 512):\n        \"\"\"\n        :param feature_map: feature map in level L\n        :param decoder_dim: size of decoder's RNN\n        \"\"\"\n        super(Channel_wise_attention, self).__init__()\n        _,C,H,W = tuple([int(x) for x in feature_map])\n        self.W_c = nn.Parameter(torch.randn(1,K))\n        self.W_hc = nn.Parameter(torch.randn(K,decoder_dim))\n        self.W_i_hat = nn.Parameter(torch.randn(K,1))\n        self.bc = nn.Parameter(torch.randn(K))\n        self.bi_hat = nn.Parameter(torch.randn(1))\n        self.tanh = nn.Tanh()\n        self.softmax = nn.Softmax(dim = 0)  # softmax layer to calculate weights\n        \n    def forward(self, feature_map, decoder_hidden):\n        \"\"\"\n        Forward propagation.\n        :param feature_map: feature map in level L(batch_size, C, H, W)\n        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n        :return: alpha\n        \"\"\"\n        V_map = feature_map.view(feature_map.shape[0],2048,-1) .mean(dim=2)\n        V_map = V_map.unsqueeze(2)#(batch_size,C,1)\n        # print(feature_map.shape)\n        # print(V_map.shape)\n        # print(\"wc\",self.W_c.shape)\n        # print(\"whc\",self.W_hc.shape)\n        # print(\"decoder_hidden\",decoder_hidden.shape)\n        # print(\"m1\",torch.matmul(V_map,self.W_c).shape)\n        # print(\"m2\",torch.matmul(decoder_hidden,self.W_hc).shape)\n        # print(\"bc\",self.bc.shape)\n        att = self.tanh((torch.matmul(V_map,self.W_c) + self.bc) + (torch.matmul(decoder_hidden,self.W_hc).unsqueeze(1)))#(batch_size,C,K)\n#         print(\"att\",att.shape)\n        beta = self.softmax(torch.matmul(att,self.W_i_hat) + self.bi_hat)\n        beta = beta.unsqueeze(2)\n        # print(\"beta\",beta.shape)\n        attention_weighted_encoding = torch.mul(feature_map,beta)\n\n        return attention_weighted_encoding,beta","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:51.773806Z","iopub.execute_input":"2023-01-03T20:30:51.774141Z","iopub.status.idle":"2023-01-03T20:30:51.784613Z","shell.execute_reply.started":"2023-01-03T20:30:51.774113Z","shell.execute_reply":"2023-01-03T20:30:51.783436Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class DecoderWithAttention(nn.Module):\n    \"\"\"\n    Decoder.\n    shift to sca attention\n    \"\"\"\n\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size,encoder_out_shape=[1,2048,8,8], K=512,encoder_dim=2048, dropout=0.5):\n        \"\"\"\n        :param attention_dim: size of attention network\n        :param embed_dim: embedding size\n        :param decoder_dim: size of decoder's RNN\n        :param vocab_size: size of vocabulary\n        :param encoder_dim: feature size of encoded images\n        :param dropout: dropout\n        \"\"\"\n        super(DecoderWithAttention, self).__init__()\n\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n\n        self.Spatial_attention = Spatial_attention(encoder_out_shape, decoder_dim, K)  # attention network\n        self.Channel_wise_attention = Channel_wise_attention(encoder_out_shape, decoder_dim, K) # ATTENTION \n        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()  # initialize some layers with the uniform distribution\n        self.AvgPool = nn.AvgPool2d(8)\n    def init_weights(self):\n        \"\"\"\n        Initializes some parameters with values from the uniform distribution, for easier convergence.\n        \"\"\"\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def load_pretrained_embeddings(self, embeddings):\n        \"\"\"\n        Loads embedding layer with pre-trained embeddings.\n        :param embeddings: pre-trained embeddings\n        \"\"\"\n        self.embedding.weight = nn.Parameter(embeddings)\n\n    def fine_tune_embeddings(self, fine_tune=True):\n        \"\"\"\n        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n        :param fine_tune: Allow?\n        \"\"\"\n        for p in self.embedding.parameters():\n            p.requires_grad = fine_tune\n\n    def init_hidden_state(self, encoder_out):\n        \"\"\"\n        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :return: hidden state, cell state\n        \"\"\"\n        mean_encoder_out = self.AvgPool(encoder_out).squeeze(-1).squeeze(-1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        \"\"\"\n        Forward propagation.\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"\n\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n\n        # Flatten image\n        # encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        # num_pixels = encoder_out.size(1)\n\n        # Sort input data by decreasing lengths; why? apparent below\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n\n        # Initialize LSTM state\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n\n        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n        # So, decoding lengths are actual lengths - 1\n        decode_lengths = (caption_lengths - 1).tolist()\n\n        # Create tensors to hold word predicion scores and alphas\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)#需要更改形状？\n        #alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)#需要更改形状\n\n        # At each time-step, decode by\n        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            # attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n            #                                                     h[:batch_size_t])\n            #channel-spatial模式attention\n            #channel_wise\n            attention_weighted_encoding, beta = self.Channel_wise_attention(encoder_out[:batch_size_t],h[:batch_size_t])\n            #spatial\n            attention_weighted_encoding, alpha = self.Spatial_attention(attention_weighted_encoding[:batch_size_t],h[:batch_size_t])\n            #对attention_weighted_encoding降维\n            attention_weighted_encoding = attention_weighted_encoding.view(attention_weighted_encoding.shape[0],2048,8,8)\n            attention_weighted_encoding = self.AvgPool(attention_weighted_encoding)\n            attention_weighted_encoding = attention_weighted_encoding.squeeze(-1).squeeze(-1)\n            # gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            # attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            #alphas[:batch_size_t, t, :] = alpha\n\n        return predictions, encoded_captions, decode_lengths, sort_ind","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:52.934340Z","iopub.execute_input":"2023-01-03T20:30:52.934706Z","iopub.status.idle":"2023-01-03T20:30:52.955662Z","shell.execute_reply.started":"2023-01-03T20:30:52.934676Z","shell.execute_reply":"2023-01-03T20:30:52.954583Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def accuracy(scores, targets, k):\n    \"\"\"\n    Computes top-k accuracy, from predicted and true labels.\n    :param scores: scores from the model\n    :param targets: true labels\n    :param k: k in top-k accuracy\n    :return: top-k accuracy\n    \"\"\"\n\n    batch_size = targets.size(0)\n    _, ind = scores.topk(k, 1, True, True)\n    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n    correct_total = correct.view(-1).float().sum()  # 0D tensor\n    return correct_total.item() * (100.0 / batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:58.785605Z","iopub.execute_input":"2023-01-03T20:30:58.786906Z","iopub.status.idle":"2023-01-03T20:30:58.793806Z","shell.execute_reply.started":"2023-01-03T20:30:58.786866Z","shell.execute_reply":"2023-01-03T20:30:58.792661Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    Clips gradients computed during backpropagation to avoid explosion of gradients.\n    :param optimizer: optimizer with the gradients to be clipped\n    :param grad_clip: clip value\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:30:59.486253Z","iopub.execute_input":"2023-01-03T20:30:59.486623Z","iopub.status.idle":"2023-01-03T20:30:59.492255Z","shell.execute_reply.started":"2023-01-03T20:30:59.486593Z","shell.execute_reply":"2023-01-03T20:30:59.491271Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def adjust_learning_rate(optimizer, shrink_factor):\n    \"\"\"\n    Shrinks learning rate by a specified factor.\n    :param optimizer: optimizer whose learning rate must be shrunk.\n    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n    \"\"\"\n\n    print(\"\\nDECAYING learning rate.\")\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = param_group['lr'] * shrink_factor\n    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:31:00.122564Z","iopub.execute_input":"2023-01-03T20:31:00.124017Z","iopub.status.idle":"2023-01-03T20:31:00.129741Z","shell.execute_reply.started":"2023-01-03T20:31:00.123978Z","shell.execute_reply":"2023-01-03T20:31:00.128749Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n                    is_best):\n    \"\"\"\n    Saves model checkpoint.\n    :param data_name: base name of processed dataset\n    :param epoch: epoch number\n    :param epochs_since_improvement: number of epochs since last improvement in BLEU-4 score\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param encoder_optimizer: optimizer to update encoder's weights, if fine-tuning\n    :param decoder_optimizer: optimizer to update decoder's weights\n    :param bleu4: validation BLEU-4 score for this epoch\n    :param is_best: is this checkpoint the best so far?\n    \"\"\"\n    state = {'epoch': epoch,\n             'epochs_since_improvement': epochs_since_improvement,\n             'encoder': encoder,\n             'decoder': decoder,\n             'encoder_optimizer': encoder_optimizer,\n             'decoder_optimizer': decoder_optimizer}\n    filename = 'checkpoint_' + data_name + '.pth.tar'\n    torch.save(state, filename)\n    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n    if is_best:\n        torch.save(state, 'BEST_' + filename)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:31:01.306796Z","iopub.execute_input":"2023-01-03T20:31:01.308036Z","iopub.status.idle":"2023-01-03T20:31:01.314867Z","shell.execute_reply.started":"2023-01-03T20:31:01.307994Z","shell.execute_reply":"2023-01-03T20:31:01.313869Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"\n    Keeps track of most recent, average, sum, and count of a metric.\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:31:02.416859Z","iopub.execute_input":"2023-01-03T20:31:02.417232Z","iopub.status.idle":"2023-01-03T20:31:02.424606Z","shell.execute_reply.started":"2023-01-03T20:31:02.417203Z","shell.execute_reply":"2023-01-03T20:31:02.423317Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n    \"\"\"\n    Performs one epoch's training.\n\n    :param train_loader: DataLoader for training data\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param criterion: loss layer\n    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n    :param decoder_optimizer: optimizer to update decoder's weights\n    :param epoch: epoch number\n    \"\"\"\n\n    decoder.train()  # train mode (dropout and batchnorm is used)\n    encoder.train()\n\n#     batch_time = AverageMeter()  # forward prop. + back prop. time\n#     data_time = AverageMeter()  # data loading time\n    losses = AverageMeter()  # loss (per word decoded)\n    top5accs = AverageMeter()  # top5 accuracy\n\n    # Batches\n    for i, (imgs, caps,caplens) in enumerate(train_loader):\n\n        # Move to GPU, if available\n        imgs = imgs.to(device)\n        caps = caps.to(device)\n\n        # Forward prop.\n        imgs = encoder(imgs)\n        scores, caps_sorted, decode_lengths, sort_ind = decoder(imgs, caps, caplens)\n\n        # Since we decoded starting with , the targets are all words after , up to \n        targets = caps_sorted[:, 1:]\n\n        # Remove timesteps that we didn't decode at, or are pads\n        # pack_padded_sequence is an easy trick to do this\n        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n        scores = scores.data\n        targets = targets.data\n        # Calculate loss\n        loss = criterion(scores, targets)\n\n        # Add doubly stochastic attention regularization\n        #loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n        # Back prop.\n        decoder_optimizer.zero_grad()\n        if encoder_optimizer is not None:\n            encoder_optimizer.zero_grad()\n        loss.backward()\n\n        # Clip gradients\n        if grad_clip is not None:\n            clip_gradient(decoder_optimizer, grad_clip)\n            if encoder_optimizer is not None:\n                clip_gradient(encoder_optimizer, grad_clip)\n\n        # Update weights\n        decoder_optimizer.step()\n        if encoder_optimizer is not None:\n            encoder_optimizer.step()\n\n        # Keep track of metrics\n        top5 = accuracy(scores, targets, 5)\n        losses.update(loss.item(), sum(decode_lengths))\n        top5accs.update(top5, sum(decode_lengths))\n\n        # Print status\n        if i % print_freq == 0:\n            print('Epoch: [{0}][{1}/{2}]\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n                                                                          loss=losses,top5=top5accs))","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:31:03.451365Z","iopub.execute_input":"2023-01-03T20:31:03.451730Z","iopub.status.idle":"2023-01-03T20:31:03.468145Z","shell.execute_reply.started":"2023-01-03T20:31:03.451702Z","shell.execute_reply":"2023-01-03T20:31:03.467170Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def validate(val_loader, encoder, decoder, criterion):\n    \"\"\"\n    Performs one epoch's validation.\n\n    :param val_loader: DataLoader for validation data.\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param criterion: loss layer\n    :return: BLEU-4 score\n    \"\"\"\n    decoder.eval()  # eval mode (no dropout or batchnorm)\n    if encoder is not None:\n        encoder.eval()\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top5accs = AverageMeter()\n\n    references = list()  # references (true captions) for calculating BLEU-4 score\n    hypotheses = list()  # hypotheses (predictions)\n\n    # explicitly disable gradient calculation to avoid CUDA memory error\n    # solves the issue #57\n    with torch.no_grad():\n        # Batches\n        for i, (imgs, caps,caplens) in enumerate(val_loader):\n\n            # Move to device, if available\n            imgs = imgs.to(device)\n            caps = caps.to(device)\n\n            # Forward prop.\n            if encoder is not None:\n                imgs = encoder(imgs)\n            scores, caps_sorted, decode_lengths, sort_ind = decoder(imgs, caps, caplens)\n\n            # Since we decoded starting with , the targets are all words after , up to \n            targets = caps_sorted[:, 1:]\n\n            # Remove timesteps that we didn't decode at, or are pads\n            # pack_padded_sequence is an easy trick to do this\n            scores_copy = scores.clone()\n            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n            scores = scores.data\n            targets = targets.data\n            # Calculate loss\n            loss = criterion(scores, targets)\n\n            # Add doubly stochastic attention regularization\n            #loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n            # Keep track of metrics\n            losses.update(loss.item(), sum(decode_lengths))\n            top5 = accuracy(scores, targets, 5)\n            top5accs.update(top5, sum(decode_lengths))\n\n\n            if i % print_freq == 0:\n                print('Validation: [{0}/{1}]\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader),loss=losses, top5=top5accs))\n\n            # Store references (true captions), and hypothesis (prediction) for each image\n            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n\n            # References\n#             allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n#             for j in range(allcaps.shape[0]):\n#                 img_caps = allcaps[j].tolist()\n#                 img_captions = list(\n#                     map(lambda c: [w for w in c if w not in {word_map[''], word_map['']}],\n#                         img_caps))  # remove  and pads\n#                 references.append(img_captions)\n\n            # Hypotheses\n            _, preds = torch.max(scores_copy, dim=2)\n            preds = preds.tolist()\n            temp_preds = list()\n            for j, p in enumerate(preds):\n                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n            preds = temp_preds\n            hypotheses.extend(preds)\n\n            #assert len(references) == len(hypotheses)\n\n        # Calculate BLEU-4 scores\n        #bleu4 = corpus_bleu(references, hypotheses)\n\n        print(\n            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}\\n'.format(\n                loss=losses,\n                top5=top5accs))\n\n    return losses.avg","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:31:12.420645Z","iopub.execute_input":"2023-01-03T20:31:12.421009Z","iopub.status.idle":"2023-01-03T20:31:12.434806Z","shell.execute_reply.started":"2023-01-03T20:31:12.420979Z","shell.execute_reply":"2023-01-03T20:31:12.433558Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"emb_dim = 512  # dimension of word embeddings\nattention_dim = 512  # dimension of attention linear layers\ndecoder_dim = 512  # dimension of decoder RNN\ndropout = 0.5\n\n# Training parameters\nstart_epoch = 0\nepochs = 2  # number of epochs to train for (if early stopping is not triggered)\nepochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\nbatch_size = 16\nencoder_lr = 1e-4  # learning rate for encoder if fine-tuning\ndecoder_lr = 4e-4  # learning rate for decoder\ngrad_clip = 5.  # clip gradients at an absolute value of\nalpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\nbest_bleu4 = 0.  # BLEU-4 score right now\nprint_freq = 100  # print training/validation stats every __ batches\nfine_tune_encoder = False  # fine-tune encoder?\ncheckpoint = None  # path to checkpoint, None if none","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:31:12.916203Z","iopub.execute_input":"2023-01-03T20:31:12.916550Z","iopub.status.idle":"2023-01-03T20:31:12.922486Z","shell.execute_reply.started":"2023-01-03T20:31:12.916521Z","shell.execute_reply":"2023-01-03T20:31:12.921535Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"word_map = train_dataloader.dataset.vocab.word2idx\ndecoder = DecoderWithAttention(attention_dim=attention_dim,\n                                       embed_dim=emb_dim,\n                                       decoder_dim=decoder_dim,\n                                       vocab_size=len(word_map),\n                                       dropout=dropout)\ndecoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),lr=decoder_lr)\nencoder = Encoder()\nencoder.fine_tune(fine_tune_encoder)\nencoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n                                     lr=encoder_lr) if fine_tune_encoder else None\n\ndecoder = decoder.to(device)\nencoder = encoder.to(device)\n\n# Loss function\ncriterion = nn.CrossEntropyLoss().to(device)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:32:11.170259Z","iopub.execute_input":"2023-01-03T20:32:11.170624Z","iopub.status.idle":"2023-01-03T20:32:22.564094Z","shell.execute_reply.started":"2023-01-03T20:32:11.170596Z","shell.execute_reply":"2023-01-03T20:32:22.562997Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/97.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"639655d735324ca1aeb8605921e5d754"}},"metadata":{}}]},{"cell_type":"code","source":"# Epochs\nfor epoch in range(start_epoch, epochs):\n    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n    if epochs_since_improvement == 20:\n        break\n    if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n        adjust_learning_rate(decoder_optimizer, 0.8)\n        if fine_tune_encoder:\n            adjust_learning_rate(encoder_optimizer, 0.8)\n\n        # One epoch's training\n    train(train_loader=train_dataloader,\n          encoder=encoder,\n          decoder=decoder,\n          criterion=criterion,\n          encoder_optimizer=encoder_optimizer,\n          decoder_optimizer=decoder_optimizer,\n          epoch=epoch)\n\n        # One epoch's validation\n    recent_bleu4 = validate(val_loader=valid_dataloader,\n                            encoder=encoder,\n                            decoder=decoder,\n                            criterion=criterion)\n\n    # Check if there was an improvement\n    is_best = recent_bleu4 > best_bleu4\n    best_bleu4 = max(recent_bleu4, best_bleu4)\n    if not is_best:\n        epochs_since_improvement += 1\n        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n    else:\n        epochs_since_improvement = 0\n\n    # Save checkpoint\n    save_checkpoint('SCA_CNN', epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n                    decoder_optimizer, is_best)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T20:32:37.990093Z","iopub.execute_input":"2023-01-03T20:32:37.990441Z","iopub.status.idle":"2023-01-03T21:18:04.496528Z","shell.execute_reply.started":"2023-01-03T20:32:37.990413Z","shell.execute_reply":"2023-01-03T21:18:04.494858Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Epoch: [0][0/795]\tLoss 8.9260 (8.9260)\tTop-5 Accuracy 0.060 (0.060)\nEpoch: [0][100/795]\tLoss 6.3344 (7.7331)\tTop-5 Accuracy 21.281 (14.296)\nEpoch: [0][200/795]\tLoss 5.7010 (6.8293)\tTop-5 Accuracy 28.099 (19.519)\nEpoch: [0][300/795]\tLoss 5.6928 (6.4424)\tTop-5 Accuracy 28.912 (22.521)\nEpoch: [0][400/795]\tLoss 5.5128 (6.2315)\tTop-5 Accuracy 30.381 (24.167)\nEpoch: [0][500/795]\tLoss 5.5507 (6.0980)\tTop-5 Accuracy 29.369 (25.176)\nEpoch: [0][600/795]\tLoss 5.4672 (6.0025)\tTop-5 Accuracy 30.190 (25.907)\nEpoch: [0][700/795]\tLoss 5.5818 (5.9325)\tTop-5 Accuracy 29.485 (26.453)\nValidation: [0/199]\tLoss 5.4317 (5.4317)\tTop-5 Accuracy 30.322 (30.322)\t\nValidation: [100/199]\tLoss 5.3572 (5.4131)\tTop-5 Accuracy 29.962 (30.543)\t\n\n * LOSS - 5.415, TOP-5 ACCURACY - 30.568\n\nEpoch: [1][0/795]\tLoss 5.3575 (5.3575)\tTop-5 Accuracy 30.554 (30.554)\nEpoch: [1][100/795]\tLoss 5.4442 (5.4696)\tTop-5 Accuracy 28.939 (29.998)\nEpoch: [1][200/795]\tLoss 5.5081 (5.4679)\tTop-5 Accuracy 30.373 (30.031)\nEpoch: [1][300/795]\tLoss 5.4243 (5.4619)\tTop-5 Accuracy 30.324 (30.101)\nEpoch: [1][400/795]\tLoss 5.3918 (5.4561)\tTop-5 Accuracy 30.123 (30.145)\nEpoch: [1][500/795]\tLoss 5.4226 (5.4525)\tTop-5 Accuracy 30.638 (30.187)\nEpoch: [1][600/795]\tLoss 5.4744 (5.4469)\tTop-5 Accuracy 28.008 (30.254)\nEpoch: [1][700/795]\tLoss 5.3466 (5.4442)\tTop-5 Accuracy 29.994 (30.274)\nValidation: [0/199]\tLoss 5.3130 (5.3130)\tTop-5 Accuracy 31.354 (31.354)\t\nValidation: [100/199]\tLoss 5.4911 (5.3569)\tTop-5 Accuracy 29.408 (30.798)\t\n\n * LOSS - 5.355, TOP-5 ACCURACY - 30.810\n\n\nEpochs since last improvement: 1\n\n","output_type":"stream"}]},{"cell_type":"code","source":"checkpoint = '/kaggle/working/BEST_checkpoint_SCA_CNN.pth.tar'  # model checkpoint\nword_map_file =  test_dataloader.dataset.vocab.word2idx\n\ncudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n\n# Load model\ncheckpoint = torch.load(checkpoint)\ndecoder1 = checkpoint['decoder']\ndecoder1 = decoder1.to(device)\ndecoder1.eval()\nencoder1 = checkpoint['encoder']\nencoder1 = encoder1.to(device)\nencoder1.eval()\n\nrev_word_map = test_dataloader.dataset.vocab.idx2word\nvocab_size = len(word_map_file)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T21:20:10.826946Z","iopub.execute_input":"2023-01-03T21:20:10.827332Z","iopub.status.idle":"2023-01-03T21:20:11.079851Z","shell.execute_reply.started":"2023-01-03T21:20:10.827295Z","shell.execute_reply":"2023-01-03T21:20:11.078828Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def evaluate(beam_size,loader):\n    \"\"\"\n    Evaluation\n    :param beam_size: beam size at which to generate captions for evaluation\n    :return: BLEU-4 score\n    \"\"\"\n    # DataLoader\n    # TODO: Batched Beam Search\n    # Therefore, do not use a batch_size greater than 1 - IMPORTANT!\n\n    # Lists to store references (true captions), and hypothesis (prediction) for each image\n    # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n    # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n    references = list()\n    hypotheses = list()\n\n    # For each image\n    for i, (orig_image, image, caps) in enumerate(tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n\n        k = beam_size\n\n        # Move to GPU device, if available\n        image = image.to(device)  # (1, 3, 256, 256)\n\n        # Encode\n        encoder_out = encoder1(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n        enc_image_size = encoder_out.size(1)\n        encoder_dim = encoder_out.size(3)\n\n        # Flatten encoding\n        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        # We'll treat the problem as having a batch size of k\n        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n\n        # Tensor to store top k previous words at each step; now they're just <start>\n        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n\n        # Tensor to store top k sequences; now they're just <start>\n        seqs = k_prev_words  # (k, 1)\n\n        # Tensor to store top k sequences' scores; now they're just 0\n        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n\n        # Lists to store completed sequences and scores\n        complete_seqs = list()\n        complete_seqs_scores = list()\n\n        # Start decoding\n        step = 1\n        h, c = decoder1.init_hidden_state(encoder_out)\n        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n        while True:\n\n            embeddings = decoder1.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n            \n            #awe, _ = decoder1.Channel_wise_attention(encoder_out,h)\n            awe, _ = decoder1.Spatial_attention(encoder_out,h)\n            awe = awe.view(awe.shape[0],2048,8,8)\n            awe = nn.AvgPool2d(8)(awe)\n            awe = awe.squeeze(-1).squeeze(-1)  # (s, encoder_dim), (s, num_pixels)\n\n            gate = decoder1.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n            awe = gate * awe\n\n            h, c = decoder1.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n\n            scores = decoder1.fc(h)  # (s, vocab_size)\n            scores = F.log_softmax(scores, dim=1)\n\n            # Add\n            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n\n            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n            if step == 1:\n                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n            else:\n                # Unroll and find top scores, and their unrolled indices\n                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n\n            # Convert unrolled indices to actual indices of scores\n            prev_word_inds = top_k_words / vocab_size  # (s)\n            next_word_inds = top_k_words % vocab_size  # (s)\n            prev_word_inds = prev_word_inds.type(torch.LongTensor)        \n            # Add new words to sequences\n            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n\n            # Which sequences are incomplete (didn't reach <end>)?\n            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n                               next_word != word_map['<end>']]\n            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n\n            # Set aside complete sequences\n            if len(complete_inds) > 0:\n                complete_seqs.extend(seqs[complete_inds].tolist())\n                complete_seqs_scores.extend(top_k_scores[complete_inds])\n            k -= len(complete_inds)  # reduce beam length accordingly\n\n            # Proceed with incomplete sequences\n            if k == 0:\n                break\n            seqs = seqs[incomplete_inds]\n            h = h[prev_word_inds[incomplete_inds]]\n            c = c[prev_word_inds[incomplete_inds]]\n            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n\n            # Break if things have been going on too long\n            if step > 50:\n                break\n            step += 1\n\n        i = complete_seqs_scores.index(max(complete_seqs_scores))\n        seq = complete_seqs[i]\n\n        # References\n#         img_caps = allcaps[0].tolist()\n#         img_captions = list(\n#             map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n#                 img_caps))  # remove <start> and pads\n#         references.append(img_captions)\n\n        # Hypotheses\n        hypotheses.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n\n\n    # Calculate BLEU-4 scores\n    return hypotheses","metadata":{"execution":{"iopub.status.busy":"2023-01-03T23:53:40.846920Z","iopub.execute_input":"2023-01-03T23:53:40.847297Z","iopub.status.idle":"2023-01-03T23:53:40.868349Z","shell.execute_reply.started":"2023-01-03T23:53:40.847267Z","shell.execute_reply":"2023-01-03T23:53:40.867167Z"},"trusted":true},"execution_count":341,"outputs":[]},{"cell_type":"code","source":"# beam_size=1\n# evaluate(beam_size,test_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T23:53:44.942187Z","iopub.execute_input":"2023-01-03T23:53:44.942567Z","iopub.status.idle":"2023-01-03T23:53:44.947151Z","shell.execute_reply.started":"2023-01-03T23:53:44.942534Z","shell.execute_reply":"2023-01-03T23:53:44.946150Z"},"trusted":true},"execution_count":342,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}