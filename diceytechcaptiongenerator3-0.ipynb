{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torchvision\nimport torch.nn.functional as F\nimport numpy as np\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport skimage.transform\nimport argparse\nfrom PIL import Image\nimport cv2\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-01-03T11:38:55.787605Z","iopub.execute_input":"2023-01-03T11:38:55.788078Z","iopub.status.idle":"2023-01-03T11:38:58.623135Z","shell.execute_reply.started":"2023-01-03T11:38:55.788042Z","shell.execute_reply":"2023-01-03T11:38:58.621943Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \"\"\"\n    Encoder.\n    shift to only output the feature map\n    \"\"\"\n\n    def __init__(self, encoded_image_size=14):\n        super(Encoder, self).__init__()\n        self.enc_image_size = encoded_image_size\n\n        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n\n        # Remove linear and pool layers (since we're not doing classification)\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n\n        # Resize image to fixed size to allow input images of variable size\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n\n        self.fine_tune()\n\n    def forward(self, images):\n        \"\"\"\n        Forward propagation.\n        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n        :return: encoded images\n        \"\"\"\n        feature_map = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n        #out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n        #out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n        return feature_map\n\n    def fine_tune(self, fine_tune=False):\n        \"\"\"\n        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n        :param fine_tune: Allow?\n        \"\"\"\n        for p in self.resnet.parameters():\n            p.requires_grad = False\n        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n        for c in list(self.resnet.children())[5:]:\n            for p in c.parameters():\n                p.requires_grad = fine_tune","metadata":{"execution":{"iopub.status.busy":"2023-01-03T11:38:58.625171Z","iopub.execute_input":"2023-01-03T11:38:58.625729Z","iopub.status.idle":"2023-01-03T11:38:58.637655Z","shell.execute_reply.started":"2023-01-03T11:38:58.625675Z","shell.execute_reply":"2023-01-03T11:38:58.636212Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Spatial_attention(nn.Module):\n    \"\"\"\n    Attention Network.\n    \"\"\"\n\n    def __init__(self,feature_map,decoder_dim,K = 512):\n        \"\"\"\n        :param feature_map: feature map in level L\n        :param decoder_dim: size of decoder's RNN\n        \"\"\"\n        super(Spatial_attention, self).__init__()\n        _,C,H,W = tuple([int(x) for x in feature_map])\n        self.W_s = nn.Parameter(torch.randn(C,K))\n        self.W_hs = nn.Parameter(torch.randn(K,decoder_dim))\n        self.W_i = nn.Parameter(torch.randn(K,1))\n        self.bs = nn.Parameter(torch.randn(K))\n        self.bi = nn.Parameter(torch.randn(1))\n        self.tanh = nn.Tanh()\n        self.softmax = nn.Softmax(dim = 0)  # softmax layer to calculate weights\n        \n    def forward(self, feature_map, decoder_hidden):\n        \"\"\"\n        Forward propagation.\n        :param feature_map: feature map in level L(batch_size, C, H, W)\n        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n        :return: alpha\n        \"\"\"\n        V_map = feature_map.view(feature_map.shape[0],2048,-1) \n        V_map = V_map.permute(0,2,1)#(batch_size,W*H,C)\n        # print(V_map.shape)\n        # print(\"m1\",torch.matmul(V_map,self.W_s).shape)\n        # print(\"m2\",torch.matmul(decoder_hidden,self.W_hs).shape)\n        att = self.tanh((torch.matmul(V_map,self.W_s)+self.bs) + (torch.matmul(decoder_hidden,self.W_hs).unsqueeze(1)))#(batch_size,W*H,C)\n        # print(\"att\",att.shape)\n        alpha = self.softmax(torch.matmul(att,self.W_i) + self.bi)\n#         print(\"alpha\",alpha.shape)\n        alpha = alpha.squeeze(2)\n        feature_map = feature_map.view(feature_map.shape[0],2048,-1) \n        # print(\"feature_map\",feature_map.shape)\n        # print(\"alpha\",alpha.shape)\n        temp_alpha = alpha.unsqueeze(1)\n        attention_weighted_encoding = torch.mul(feature_map,temp_alpha)\n        return attention_weighted_encoding,alpha","metadata":{"execution":{"iopub.status.busy":"2023-01-03T11:38:58.770135Z","iopub.execute_input":"2023-01-03T11:38:58.770525Z","iopub.status.idle":"2023-01-03T11:38:58.782041Z","shell.execute_reply.started":"2023-01-03T11:38:58.770494Z","shell.execute_reply":"2023-01-03T11:38:58.781054Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Channel_wise_attention(nn.Module):\n    \"\"\"\n    Attention Network.\n    \"\"\"\n\n    def __init__(self,feature_map,decoder_dim,K = 512):\n        \"\"\"\n        :param feature_map: feature map in level L\n        :param decoder_dim: size of decoder's RNN\n        \"\"\"\n        super(Channel_wise_attention, self).__init__()\n        _,C,H,W = tuple([int(x) for x in feature_map])\n        self.W_c = nn.Parameter(torch.randn(1,K))\n        self.W_hc = nn.Parameter(torch.randn(K,decoder_dim))\n        self.W_i_hat = nn.Parameter(torch.randn(K,1))\n        self.bc = nn.Parameter(torch.randn(K))\n        self.bi_hat = nn.Parameter(torch.randn(1))\n        self.tanh = nn.Tanh()\n        self.softmax = nn.Softmax(dim = 0)  # softmax layer to calculate weights\n        \n    def forward(self, feature_map, decoder_hidden):\n        \"\"\"\n        Forward propagation.\n        :param feature_map: feature map in level L(batch_size, C, H, W)\n        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n        :return: alpha\n        \"\"\"\n        V_map = feature_map.view(feature_map.shape[0],2048,-1) .mean(dim=2)\n        V_map = V_map.unsqueeze(2)#(batch_size,C,1)\n        # print(feature_map.shape)\n        # print(V_map.shape)\n        # print(\"wc\",self.W_c.shape)\n        # print(\"whc\",self.W_hc.shape)\n        # print(\"decoder_hidden\",decoder_hidden.shape)\n        # print(\"m1\",torch.matmul(V_map,self.W_c).shape)\n        # print(\"m2\",torch.matmul(decoder_hidden,self.W_hc).shape)\n        # print(\"bc\",self.bc.shape)\n        att = self.tanh((torch.matmul(V_map,self.W_c) + self.bc) + (torch.matmul(decoder_hidden,self.W_hc).unsqueeze(1)))#(batch_size,C,K)\n#         print(\"att\",att.shape)\n        beta = self.softmax(torch.matmul(att,self.W_i_hat) + self.bi_hat)\n        beta = beta.unsqueeze(2)\n        # print(\"beta\",beta.shape)\n        attention_weighted_encoding = torch.mul(feature_map,beta)\n\n        return attention_weighted_encoding,beta","metadata":{"execution":{"iopub.status.busy":"2023-01-03T11:38:59.406841Z","iopub.execute_input":"2023-01-03T11:38:59.407450Z","iopub.status.idle":"2023-01-03T11:38:59.419186Z","shell.execute_reply.started":"2023-01-03T11:38:59.407416Z","shell.execute_reply":"2023-01-03T11:38:59.417766Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class DecoderWithAttention(nn.Module):\n    \"\"\"\n    Decoder.\n    shift to sca attention\n    \"\"\"\n\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size,encoder_out_shape=[1,2048,8,8], K=512,encoder_dim=2048, dropout=0.5):\n        \"\"\"\n        :param attention_dim: size of attention network\n        :param embed_dim: embedding size\n        :param decoder_dim: size of decoder's RNN\n        :param vocab_size: size of vocabulary\n        :param encoder_dim: feature size of encoded images\n        :param dropout: dropout\n        \"\"\"\n        super(DecoderWithAttention, self).__init__()\n\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n\n        self.Spatial_attention = Spatial_attention(encoder_out_shape, decoder_dim, K)  # attention network\n        self.Channel_wise_attention = Channel_wise_attention(encoder_out_shape, decoder_dim, K) # ATTENTION \n        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()  # initialize some layers with the uniform distribution\n        self.AvgPool = nn.AvgPool2d(8)\n    def init_weights(self):\n        \"\"\"\n        Initializes some parameters with values from the uniform distribution, for easier convergence.\n        \"\"\"\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def load_pretrained_embeddings(self, embeddings):\n        \"\"\"\n        Loads embedding layer with pre-trained embeddings.\n        :param embeddings: pre-trained embeddings\n        \"\"\"\n        self.embedding.weight = nn.Parameter(embeddings)\n\n    def fine_tune_embeddings(self, fine_tune=True):\n        \"\"\"\n        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n        :param fine_tune: Allow?\n        \"\"\"\n        for p in self.embedding.parameters():\n            p.requires_grad = fine_tune\n\n    def init_hidden_state(self, encoder_out):\n        \"\"\"\n        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :return: hidden state, cell state\n        \"\"\"\n        mean_encoder_out = self.AvgPool(encoder_out).squeeze(-1).squeeze(-1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        \"\"\"\n        Forward propagation.\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"\n\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n\n        # Flatten image\n        # encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        # num_pixels = encoder_out.size(1)\n\n        # Sort input data by decreasing lengths; why? apparent below\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n\n        # Initialize LSTM state\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n\n        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n        # So, decoding lengths are actual lengths - 1\n        decode_lengths = (caption_lengths - 1).tolist()\n\n        # Create tensors to hold word predicion scores and alphas\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)#需要更改形状？\n        #alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)#需要更改形状\n\n        # At each time-step, decode by\n        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            # attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n            #                                                     h[:batch_size_t])\n            #channel-spatial模式attention\n            #channel_wise\n            attention_weighted_encoding, beta = self.Channel_wise_attention(encoder_out[:batch_size_t],h[:batch_size_t])\n            #spatial\n            attention_weighted_encoding, alpha = self.Spatial_attention(attention_weighted_encoding[:batch_size_t],h[:batch_size_t])\n            #对attention_weighted_encoding降维\n            attention_weighted_encoding = attention_weighted_encoding.view(attention_weighted_encoding.shape[0],2048,8,8)\n            attention_weighted_encoding = self.AvgPool(attention_weighted_encoding)\n            attention_weighted_encoding = attention_weighted_encoding.squeeze(-1).squeeze(-1)\n            # gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            # attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            #alphas[:batch_size_t, t, :] = alpha\n\n        return predictions, encoded_captions, decode_lengths, sort_ind","metadata":{"execution":{"iopub.status.busy":"2023-01-03T11:39:00.490276Z","iopub.execute_input":"2023-01-03T11:39:00.490713Z","iopub.status.idle":"2023-01-03T11:39:00.514400Z","shell.execute_reply.started":"2023-01-03T11:39:00.490665Z","shell.execute_reply":"2023-01-03T11:39:00.513268Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3):\n    \"\"\"\n    Reads an image and captions it with beam search.\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param image_path: path to image\n    :param word_map: word map\n    :param beam_size: number of sequences to consider at each decode-step\n    :return: caption, weights for visualization\n    \"\"\"\n\n    k = beam_size\n    vocab_size = len(word_map)\n\n    # Read image and process\n    img = cv2.imread(image_path)\n    if len(img.shape) == 2:\n        img = img[:, :, np.newaxis]\n        img = np.concatenate([img, img, img], axis=2)\n    img = cv2.resize(img, (256, 256))\n    img = img.transpose(2, 0, 1)\n    img = img / 255.\n    img = torch.FloatTensor(img).to(device)\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    transform = transforms.Compose([normalize])\n    image = transform(img)  # (3, 256, 256)\n\n    # Encode\n    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n    enc_image_size = encoder_out.size(1)\n    encoder_dim = encoder_out.size(3)\n\n    # Flatten encoding\n    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n    num_pixels = encoder_out.size(1)\n\n    # We'll treat the problem as having a batch size of k\n    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n\n    # Tensor to store top k previous words at each step; now they're just <start>\n    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n\n    # Tensor to store top k sequences; now they're just <start>\n    seqs = k_prev_words  # (k, 1)\n\n    # Tensor to store top k sequences' scores; now they're just 0\n    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n\n    # Tensor to store top k sequences' alphas; now they're just 1s\n    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n\n    # Lists to store completed sequences, their alphas and scores\n    complete_seqs = list()\n    complete_seqs_alpha = list()\n    complete_seqs_scores = list()\n\n    # Start decoding\n    step = 1\n    h, c = decoder.init_hidden_state(encoder_out)\n\n    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n    while True:\n\n        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n\n        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n\n        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n\n        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n        awe = gate * awe\n\n        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n\n        scores = decoder.fc(h)  # (s, vocab_size)\n        scores = F.log_softmax(scores, dim=1)\n\n        # Add\n        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n\n        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n        if step == 1:\n            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n        else:\n            # Unroll and find top scores, and their unrolled indices\n            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n\n        # Convert unrolled indices to actual indices of scores\n        prev_word_inds = top_k_words / vocab_size  # (s)\n        next_word_inds = top_k_words % vocab_size  # (s)\n\n        # Add new words to sequences, alphas\n        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n\n        # Which sequences are incomplete (didn't reach <end>)?\n        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n                           next_word != word_map['<end>']]\n        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n\n        # Set aside complete sequences\n        if len(complete_inds) > 0:\n            complete_seqs.extend(seqs[complete_inds].tolist())\n            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n            complete_seqs_scores.extend(top_k_scores[complete_inds])\n        k -= len(complete_inds)  # reduce beam length accordingly\n\n        # Proceed with incomplete sequences\n        if k == 0:\n            break\n        seqs = seqs[incomplete_inds]\n        seqs_alpha = seqs_alpha[incomplete_inds]\n        h = h[prev_word_inds[incomplete_inds]]\n        c = c[prev_word_inds[incomplete_inds]]\n        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n\n        # Break if things have been going on too long\n        if step > 50:\n            break\n        step += 1\n\n    i = complete_seqs_scores.index(max(complete_seqs_scores))\n    seq = complete_seqs[i]\n    alphas = complete_seqs_alpha[i]\n\n    return seq, alphas\n\n\ndef visualize_att(image_path, seq, alphas, rev_word_map, smooth=True):\n    \"\"\"\n    Visualizes caption with weights at every word.\n    Adapted from paper authors' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n    :param image_path: path to image that has been captioned\n    :param seq: caption\n    :param alphas: weights\n    :param rev_word_map: reverse word mapping, i.e. ix2word\n    :param smooth: smooth weights?\n    \"\"\"\n    image = Image.open(image_path)\n    image = image.resize([14 * 24, 14 * 24], Image.LANCZOS)\n\n    words = [rev_word_map[ind] for ind in seq]\n\n    for t in range(len(words)):\n        if t > 50:\n            break\n        plt.subplot(np.ceil(len(words) / 5.), 5, t + 1)\n\n        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12)\n        plt.imshow(image)\n        current_alpha = alphas[t, :]\n        if smooth:\n            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=24, sigma=8)\n        else:\n            alpha = skimage.transform.resize(current_alpha.numpy(), [14 * 24, 14 * 24])\n        if t == 0:\n            plt.imshow(alpha, alpha=0)\n        else:\n            plt.imshow(alpha, alpha=0.8)\n        plt.set_cmap(cm.Greys_r)\n        plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-03T11:39:01.629726Z","iopub.execute_input":"2023-01-03T11:39:01.630119Z","iopub.status.idle":"2023-01-03T11:39:01.660165Z","shell.execute_reply.started":"2023-01-03T11:39:01.630088Z","shell.execute_reply":"2023-01-03T11:39:01.658826Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# # Load model\n# checkpoint = torch.load(args.model)\n# decoder = checkpoint['decoder']\n# decoder = decoder.to(device)\n# decoder.eval()\n# encoder = checkpoint['encoder']\n# encoder = encoder.to(device)\n# encoder.eval()\n\n# # Load word map (word2ix)\n# with open(args.word_map, 'r') as j:\n#     word_map = json.load(j)\n# rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n\n# # Encode, decode with attention and beam search\n# seq, alphas = caption_image_beam_search(encoder, decoder, args.img, word_map, args.beam_size)\n# alphas = torch.FloatTensor(alphas)\n\n# # Visualize caption and attention of best sequence\n# visualize_att(args.img, seq, alphas, rev_word_map, args.smooth)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T11:39:41.438754Z","iopub.execute_input":"2023-01-03T11:39:41.439179Z","iopub.status.idle":"2023-01-03T11:39:41.445143Z","shell.execute_reply.started":"2023-01-03T11:39:41.439145Z","shell.execute_reply":"2023-01-03T11:39:41.443643Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}