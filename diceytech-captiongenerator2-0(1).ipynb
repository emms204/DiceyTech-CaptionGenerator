{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"ed8ec78b91844518a60fbc040df4a708":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_818dd1c7918c40f2821f76d4b62d1107","IPY_MODEL_cf4deeda93c2443caaf9a69eb1ea1623","IPY_MODEL_49895386db4d494ebc32843aedb97b50"],"layout":"IPY_MODEL_f4da2e3e7a97402085c50ed3098b3ed3"}},"818dd1c7918c40f2821f76d4b62d1107":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24632c50ed2f46a2a05eb51011f998bf","placeholder":"​","style":"IPY_MODEL_4fa7ab077d704df4b008cd67a4bff6d0","value":"  0%"}},"cf4deeda93c2443caaf9a69eb1ea1623":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_7686cef7548d4bdf936f42bf8002826a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed57442cc5ca481daa3cc517d7f392c2","value":0}},"49895386db4d494ebc32843aedb97b50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6a583f6d36941d784c3d668512e2f6d","placeholder":"​","style":"IPY_MODEL_9d36fab3e81348ca90b38b81efa3029b","value":" 0/1 [00:00&lt;?, ?it/s]"}},"f4da2e3e7a97402085c50ed3098b3ed3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24632c50ed2f46a2a05eb51011f998bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fa7ab077d704df4b008cd67a4bff6d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7686cef7548d4bdf936f42bf8002826a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed57442cc5ca481daa3cc517d7f392c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6a583f6d36941d784c3d668512e2f6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d36fab3e81348ca90b38b81efa3029b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n# os.chdir('/content/drive/MyDrive/Flickr Image Dataset')","metadata":{"id":"4qdlRHER0tAC","execution":{"iopub.status.busy":"2023-01-04T03:27:15.942714Z","iopub.execute_input":"2023-01-04T03:27:15.943073Z","iopub.status.idle":"2023-01-04T03:27:15.964849Z","shell.execute_reply.started":"2023-01-04T03:27:15.942972Z","shell.execute_reply":"2023-01-04T03:27:15.964066Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"INPUT_IMAGES_DIR = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/\"\nLABEL_PATH = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\nOUTPUT_PATH = \"/kaggle/working\"","metadata":{"execution":{"iopub.status.busy":"2023-01-04T03:27:16.084506Z","iopub.execute_input":"2023-01-04T03:27:16.084777Z","iopub.status.idle":"2023-01-04T03:27:16.089391Z","shell.execute_reply.started":"2023-01-04T03:27:16.084752Z","shell.execute_reply":"2023-01-04T03:27:16.088234Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport gc\nimport numpy as np\nimport pandas as pd\nimport itertools\nfrom tqdm.autonotebook import tqdm\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport pickle\nfrom torch.nn.utils.rnn import pad_sequence\n\nimport torch\nimport torchvision\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom collections import Counter\nimport nltk\nnltk.download('punkt')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jo_jiH7804hw","outputId":"c44b6122-a0d0-467c-8ee2-cf3ded018ac1","execution":{"iopub.status.busy":"2023-01-04T03:27:16.354362Z","iopub.execute_input":"2023-01-04T03:27:16.354926Z","iopub.status.idle":"2023-01-04T03:27:19.840078Z","shell.execute_reply.started":"2023-01-04T03:27:16.354886Z","shell.execute_reply":"2023-01-04T03:27:19.839198Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  import sys\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv(LABEL_PATH, delimiter=\"|\")\ndf.columns = ['image', 'caption_number', 'caption']\ndf['caption'] = df['caption'].str.lstrip()\ndf['caption_number'] = df['caption_number'].str.lstrip()\ndf.loc[19999, 'caption_number'] = \"4\"\ndf.loc[19999, 'caption'] = \"A dog runs across the grass .\"\nids = [id_ for id_ in range(len(df) // 5) for i in range(5)]\ndf['id'] = ids\ndf.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"QxIF472H1AWd","outputId":"42de6949-5a23-48df-b2c7-c27b4f3b4b85","execution":{"iopub.status.busy":"2023-01-04T03:27:19.842185Z","iopub.execute_input":"2023-01-04T03:27:19.842720Z","iopub.status.idle":"2023-01-04T03:27:20.453742Z","shell.execute_reply.started":"2023-01-04T03:27:19.842682Z","shell.execute_reply":"2023-01-04T03:27:20.452849Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"            image caption_number  \\\n0  1000092795.jpg              0   \n1  1000092795.jpg              1   \n2  1000092795.jpg              2   \n3  1000092795.jpg              3   \n4  1000092795.jpg              4   \n\n                                             caption  id  \n0  Two young guys with shaggy hair look at their ...   0  \n1  Two young , White males are outside near many ...   0  \n2   Two men in green shirts are standing in a yard .   0  \n3       A man in a blue shirt standing in a garden .   0  \n4            Two friends enjoy time spent together .   0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption_number</th>\n      <th>caption</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000092795.jpg</td>\n      <td>0</td>\n      <td>Two young guys with shaggy hair look at their ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000092795.jpg</td>\n      <td>1</td>\n      <td>Two young , White males are outside near many ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000092795.jpg</td>\n      <td>2</td>\n      <td>Two men in green shirts are standing in a yard .</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000092795.jpg</td>\n      <td>3</td>\n      <td>A man in a blue shirt standing in a garden .</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000092795.jpg</td>\n      <td>4</td>\n      <td>Two friends enjoy time spent together .</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df[\"length\"] = df[\"caption\"].apply(lambda row: len(row.strip().split()))","metadata":{"id":"M54CyqMh7bJ5","execution":{"iopub.status.busy":"2023-01-04T03:27:20.455397Z","iopub.execute_input":"2023-01-04T03:27:20.460734Z","iopub.status.idle":"2023-01-04T03:27:20.640747Z","shell.execute_reply.started":"2023-01-04T03:27:20.460687Z","shell.execute_reply":"2023-01-04T03:27:20.639801Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom collections import Counter\n\nclass Vocabulary():\n    def __init__(self, df,vocab_threshold,vocab_file='vocab.pkl',\n               start_word=\"<start>\",end_word=\"<end>\",unk_word=\"<unk>\",pad=\"<pad>\",vocab_from_file=False):\n        self.vocab_threshold = vocab_threshold\n        self.vocab_file = vocab_file\n        self.start_word = start_word\n        self.end_word = end_word\n        self.unk_word = unk_word\n        self.pad_word = pad\n        self.vocab_from_file = vocab_from_file\n        self.df = df\n        self.get_vocab()\n        \n    def get_vocab(self):\n        \"\"\"Load the vocabulary from file OR build the vocabulary from scratch.\"\"\"\n        if os.path.exists(self.vocab_file) & self.vocab_from_file:\n            with open(self.vocab_file, 'rb') as f:\n                vocab = pickle.load(f)\n                self.word2idx = vocab.word2idx\n                self.idx2word = vocab.idx2word\n            print('Vocabulary successfully loaded from vocab.pkl file!')\n        else:\n            self.build_vocab()\n            with open(self.vocab_file, 'wb') as f:\n                pickle.dump(self, f)\n                \n    def build_vocab(self):\n        \"\"\"Populate the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n        self.init_vocab()\n        self.add_word(self.start_word)\n        self.add_word(self.end_word)\n        self.add_word(self.unk_word)\n        self.add_captions()\n\n    def init_vocab(self):\n        \"\"\"Initialize the dictionaries for converting tokens to integers (and vice versa).\"\"\"\n        self.word2idx = {}\n        self.idx2word = {}\n        self.idx = 0 \n\n    def add_word(self, word):\n        \"\"\"Add a token to the vocabulary.\"\"\"\n        if not word in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n\n    def add_captions(self):\n        \"\"\"Loop over training captions and add all tokens to the vocabulary that meet or exceed the threshold.\"\"\"\n        counter = Counter()\n        x = df['caption'].apply(lambda row:counter.update(nltk.tokenize.word_tokenize(row)))\n        words = [word for word, cnt in counter.items() if cnt>=self.vocab_threshold]\n        for i, word in enumerate(words):\n            self.add_word(word)\n\n    def __call__(self, word):\n        if not word in self.word2idx:\n            return self.word2idx[self.unk_word]\n        return self.word2idx[word]\n    \n    def __len__(self):\n        return len(self.word2idx)","metadata":{"id":"hFWVO1X57mQR","execution":{"iopub.status.busy":"2023-01-04T03:27:20.645058Z","iopub.execute_input":"2023-01-04T03:27:20.645355Z","iopub.status.idle":"2023-01-04T03:27:20.659393Z","shell.execute_reply.started":"2023-01-04T03:27:20.645328Z","shell.execute_reply":"2023-01-04T03:27:20.658331Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def collate_batch(batch):\n    image_list,caption_list,caplen_list = [],[],[]\n    for (image, caption,caplen) in batch:\n        image_list.append(image)\n        caption_list.append(caption)\n        caplen_list.append(caplen)\n    \n    image_list = torch.stack(image_list)\n    caplen_list = torch.tensor(caplen_list)\n    caption_list = pad_sequence(caption_list, batch_first=True, padding_value=0)\n    values, ind = torch.sort(caplen_list,dim=-1,descending=True)\n    return image_list,caption_list[ind],values","metadata":{"execution":{"iopub.status.busy":"2023-01-04T03:27:20.660762Z","iopub.execute_input":"2023-01-04T03:27:20.661216Z","iopub.status.idle":"2023-01-04T03:27:20.672345Z","shell.execute_reply.started":"2023-01-04T03:27:20.661182Z","shell.execute_reply":"2023-01-04T03:27:20.671315Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class ImageCaptioningDataset(torch.utils.data.Dataset):\n    def __init__(self,df,transform,mode,batch_size,vocab_threshold):\n        self.caption_lengths = df['length']\n        self.batch_size = batch_size\n        self.df = df\n        self.transform = transform\n        self.mode = mode\n        if mode == 'train':\n            self.vocab = Vocabulary(df,vocab_threshold,'vocab.pkl1',vocab_from_file=False)\n        elif (mode=='val') or (mode=='test'):\n            self.vocab = Vocabulary(df,vocab_threshold,'vocab_pkl1',vocab_from_file=True)\n    def __getitem__(self,index):\n        if (self.mode == 'train') or (self.mode =='val'):\n            image = Image.open(f\"{INPUT_IMAGES_DIR}/{self.df['image'][index]}\")\n            image = self.transform(image)\n\n            #Convert caption to tensor of word ids \n            tokens = nltk.tokenize.word_tokenize(self.df['caption'][index].lower())\n            caption = []\n            caption.append(self.vocab(self.vocab.start_word))\n            caption.extend([self.vocab(token) for token in tokens])\n            caption.append(self.vocab(self.vocab.end_word))\n            caption = torch.Tensor(caption).long()\n\n            caplen = self.df['length'][index]+2\n            return (image,caption,caplen)\n\n        else:\n            p_image = Image.open(f\"{INPUT_IMAGES_DIR}/{self.df['image'][index]}\").convert('RGB')\n            image = np.array(p_image)\n            trans_image = self.transform(p_image)\n\n            return image, trans_image,self.df['caption'][index]\n\n    def get_train_indices(self):\n        sel_length = np.random.choice(self.caption_lengths)\n        all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]\n        indices = list(np.random.choice(all_indices, size=self.batch_size))\n        return indices\n\n    def __len__(self):\n        return len(self.df['caption'])","metadata":{"id":"6FvrdOXHeDp-","execution":{"iopub.status.busy":"2023-01-04T03:27:20.675648Z","iopub.execute_input":"2023-01-04T03:27:20.675909Z","iopub.status.idle":"2023-01-04T03:27:20.690773Z","shell.execute_reply.started":"2023-01-04T03:27:20.675886Z","shell.execute_reply":"2023-01-04T03:27:20.689819Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"mode ='train'\n## TODO #1: Select appropriate values for the Python variables below.\nbatch_size = 128         # batch size\nvocab_threshold = 6        # minimum word count threshold\nembed_size = 512           # dimensionality of image and word embeddings\nhidden_size = 512          # number of features in hidden state of the RNN decoder\nnum_epochs = 1             # number of training epochs (1 for testing)\nsave_every = 1             # determines frequency of saving model weights\nprint_every = 200          # determines window for printing average loss\nlog_file = 'training_log.txt'       # name of file with saved training loss and perplexity","metadata":{"id":"goDImMexmnoZ","execution":{"iopub.status.busy":"2023-01-04T03:27:20.692584Z","iopub.execute_input":"2023-01-04T03:27:20.692983Z","iopub.status.idle":"2023-01-04T03:27:20.703561Z","shell.execute_reply.started":"2023-01-04T03:27:20.692950Z","shell.execute_reply":"2023-01-04T03:27:20.702478Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"transform_train = transforms.Compose([ \n    transforms.Resize(256),                          # smaller edge of image resized to 256\n    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n    ])\ntransform_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    ])","metadata":{"id":"wXb5M6Vbg74m","execution":{"iopub.status.busy":"2023-01-04T03:27:20.704990Z","iopub.execute_input":"2023-01-04T03:27:20.705526Z","iopub.status.idle":"2023-01-04T03:27:20.717264Z","shell.execute_reply.started":"2023-01-04T03:27:20.705492Z","shell.execute_reply":"2023-01-04T03:27:20.716292Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train, test =train_test_split(df,test_size=0.2,shuffle=True)\ntrain_df,valid_df = train_test_split(train,test_size=0.2,shuffle=True)\ntrain_df, valid_df, test = train_df.reset_index(drop=True), valid_df.reset_index(drop=True), test.reset_index(drop=True)\n\ntrain_dataset = ImageCaptioningDataset(train_df,transform_train,mode,batch_size,vocab_threshold)\nvalid_dataset = ImageCaptioningDataset(valid_df,transform_train,'val',batch_size,vocab_threshold)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True,\n                                               collate_fn=collate_batch,num_workers=2)\nvalid_dataloader = torch.utils.data.DataLoader(valid_dataset,batch_size=batch_size,shuffle=True,\n                                               collate_fn=collate_batch,num_workers=2)\n# indices = train_dataset.get_train_indices()\n# initial_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices=indices)\n#train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_sampler=torch.utils.data.sampler.BatchSampler(sampler=initial_sampler,batch_size=batch_size,drop_last=False))","metadata":{"id":"FK2Gh7eblP_v","execution":{"iopub.status.busy":"2023-01-04T03:27:20.718837Z","iopub.execute_input":"2023-01-04T03:27:20.719223Z","iopub.status.idle":"2023-01-04T03:27:56.871593Z","shell.execute_reply.started":"2023-01-04T03:27:20.719190Z","shell.execute_reply":"2023-01-04T03:27:56.870617Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"test_dataset = ImageCaptioningDataset(test,transform_test,'test',1,0)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=1,shuffle=True)","metadata":{"id":"8s4VDQW2uX2n","execution":{"iopub.status.busy":"2023-01-04T03:27:56.875239Z","iopub.execute_input":"2023-01-04T03:27:56.876011Z","iopub.status.idle":"2023-01-04T03:27:56.897399Z","shell.execute_reply.started":"2023-01-04T03:27:56.875982Z","shell.execute_reply":"2023-01-04T03:27:56.896411Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Vocabulary successfully loaded from vocab.pkl file!\n","output_type":"stream"}]},{"cell_type":"code","source":"len(train_dataloader.dataset.vocab),len(valid_dataloader.dataset.vocab),len(test_dataloader.dataset.vocab)","metadata":{"execution":{"iopub.status.busy":"2023-01-04T03:27:56.898897Z","iopub.execute_input":"2023-01-04T03:27:56.899258Z","iopub.status.idle":"2023-01-04T03:27:56.907953Z","shell.execute_reply.started":"2023-01-04T03:27:56.899223Z","shell.execute_reply":"2023-01-04T03:27:56.907083Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(7463, 7463, 7463)"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"Haeq5HFKtuKP","execution":{"iopub.status.busy":"2023-01-04T03:27:56.909196Z","iopub.execute_input":"2023-01-04T03:27:56.909635Z","iopub.status.idle":"2023-01-04T03:27:56.986760Z","shell.execute_reply.started":"2023-01-04T03:27:56.909578Z","shell.execute_reply":"2023-01-04T03:27:56.985749Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        super(EncoderCNN, self).__init__()\n        resnet = torchvision.models.resnet50(pretrained='True')\n        for param in resnet.parameters():\n            param.requires_grad_(False)\n\n        modules = list(resnet.children())[:-1]\n        self.resnet = nn.Sequential(*modules)\n        self.embed = nn.Linear(resnet.fc.in_features,embed_size)\n\n  \n    def forward(self, images):\n        features = self.resnet(images)\n        features = features.view(features.size(0),-1)\n        features = self.embed(features)\n        return features","metadata":{"id":"cbIUye971HgZ","execution":{"iopub.status.busy":"2023-01-04T03:27:56.988297Z","iopub.execute_input":"2023-01-04T03:27:56.988656Z","iopub.status.idle":"2023-01-04T03:27:56.996793Z","shell.execute_reply.started":"2023-01-04T03:27:56.988623Z","shell.execute_reply":"2023-01-04T03:27:56.996092Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self,embed_size,hidden_size,vocab_size):\n        super(DecoderRNN,self).__init__()\n        # The LSTM takes embedded word vectors (of a specified size) as input\n        # and outputs hidden states of size hidden_dim\n        self.lstm = nn.LSTM(input_size=embed_size,hidden_size=hidden_size, # LSTM hidden units \n                            num_layers=1, # number of LSTM layer\n                            bias=True,# use bias weights b_ih and b_hh\n                            batch_first=True,# input & output will have batch size as 1st dimension\n                            dropout=0, # Not applying dropout \n                            bidirectional=False) # unidirectional LSTM\n        #Embedding layer that turns words into a vector of a specified size\n        self.embed = nn.Embedding(vocab_size,embed_size,padding_idx=0)\n        # The linear layer that maps the hidden state output dimension\n        # to the number of words we want as output, vocab_size\n        self.linear = nn.Linear(hidden_size,vocab_size)\n  \n    def init_hidden(self, batch_size):\n        \"\"\" At the start of training, we need to initialize a hidden state;\n        there will be none because the hidden state is formed based on previously seen data.\n        So, this function defines a hidden state with all zeroes\n        The axes semantics are (num_layers, batch_size, hidden_dim)\n        \"\"\"\n        return (torch.zeros((1,batch_size,hidden_size),device=device),\n                torch.zeros((1,batch_size,hidden_size),device=device))\n\n    def forward(self, features, captions,captions_length):\n        \"\"\" Define the feedforward behavior of the model \"\"\"\n      \n        # Discard the <end> word to avoid predicting when <end> is the input of the RNN\n        captions = captions[:,:-1]\n        # Initialize the hidden state\n        batch_size = features.shape[0] # features is of shape (batch_size, embed_size)\n        hidden = self.init_hidden(batch_size)\n        # Create embedded word vectors for each word in the captions\n        embeddings = self.embed(captions)# embeddings new shape : (batch_size, captions length - 1, embed_size)\n        # Stack the features and captions\n        embeddings = torch.cat((features.unsqueeze(1),embeddings),dim=1)\n        embeddings = torch.nn.utils.rnn.pack_padded_sequence(embeddings, captions_length, batch_first=True)\n        # embeddings new shape : (batch_size, caption length, embed_size)\n        \n        # Get the output and hidden state by passing the lstm over our word embeddings\n        # the lstm takes in our embeddings and hidden state\n        lstm_out, hidden = self.lstm(embeddings, hidden) # lstm_out shape : (batch_size, caption length, hidden_size)\n        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n        # Fully connected layer\n        outputs = self.linear(lstm_out)# outputs shape : (batch_size, caption length, vocab_size)\n        return outputs\n\n  ##Greedy search \n    def sample(self,inputs):\n        \"accepts preprocessed image tensor(inputs) and returns predicted sentence(list of tensor ids of length max_len)\"\n        output = []\n        batch_size = inputs.shape[0] # batch_size is 1 at inference, inputs shape : (1, 1, embed_size)\n        hidden = self.init_hidden(batch_size)# Get initial hidden state of the LSTM\n        while True:\n            lstm_out, hidden = self.lstm(inputs,hidden) # lstm_out shape : (1, 1, hidden_size)\n            outputs = self.linear(lstm_out) # outputs shape : (1, 1, vocab_size)\n            outputs = outputs.squeeze(1) # outputs shape : (1, vocab_size)\n            _, max_indice = torch.max(outputs,dim=1) #predict the most likely next word, max_indice shape:(1)\n            output.append(max_indice.cpu().numpy()[0].item())# storing the word predicted\n            if(max_indice == 1):\n                # We predicted the <end> word, so there is no further prediction to do\n                break\n            ## Prepare to embed the last predicted word to be the new input of the lstm\n            inputs = self.embed(max_indice) # inputs shape : (1, embed_size)\n            inputs = inputs.unsqueeze(1) # inputs shape : (1, 1, embed_size)\n            \n        return output\n","metadata":{"id":"oTV_WU5L3LWl","execution":{"iopub.status.busy":"2023-01-04T03:27:56.999105Z","iopub.execute_input":"2023-01-04T03:27:56.999717Z","iopub.status.idle":"2023-01-04T03:27:57.014001Z","shell.execute_reply.started":"2023-01-04T03:27:56.999683Z","shell.execute_reply":"2023-01-04T03:27:57.013277Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import math\n#The size of the vocabulary.\nvocab_size = len(train_dataloader.dataset.vocab)\n\n# Initialize the encoder and decoder. \nencoder = EncoderCNN(embed_size)\ndecoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n\n# Move models to GPU if CUDA is available. \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nencoder.to(device)\ndecoder.to(device)\n\n# Define the loss function. \ncriterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n\n# TODO #3: Specify the learnable parameters of the model.\nparams = list(decoder.parameters()) + list(encoder.embed.parameters())\n\n# TODO #4: Define the optimizer.\noptimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n# optimizer = torch.optim.Adam(params, lr=0.01, betas=(0.9, 0.999), eps=1e-08)\n# optimizer = torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08)\n\n# Set the total number of training steps per epoch.\ntotal_step = math.ceil(len(train_dataloader.dataset.caption_lengths) / batch_size)\nval_step = math.ceil(len(valid_dataloader.dataset.caption_lengths)/batch_size)","metadata":{"id":"BzRQ9c7owW6x","execution":{"iopub.status.busy":"2023-01-04T03:27:57.017141Z","iopub.execute_input":"2023-01-04T03:27:57.017873Z","iopub.status.idle":"2023-01-04T03:28:05.852239Z","shell.execute_reply.started":"2023-01-04T03:27:57.017839Z","shell.execute_reply":"2023-01-04T03:28:05.851221Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/97.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7af0bfecbbdd42459fa26f5368db8f44"}},"metadata":{}}]},{"cell_type":"code","source":"for epoch in range(1, num_epochs+1):\n    for i_step in range(1, total_step+1):\n        encoder.train()\n        decoder.train()\n        # Randomly sample a caption length, and sample indices with \n#         indices = train_dataloader.dataset.get_train_indices()\n#         # Create and assign a batch sampler to retrieve a batch with the sampled \n#         new_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices=indices)\n#         train_dataloader.batch_sampler.sampler = new_sampler\n        #obtain the batch\n        images, captions, caplens = next(iter(train_dataloader))\n        images = images.to(device)\n        captions = captions.to(device)\n#         caplens = caplens.to(device)\n\n        decoder.zero_grad()\n        encoder.zero_grad()\n\n        features = encoder(images)\n        outputs = decoder(features, captions,caplens)\n        \n        if outputs.shape[1] != captions.shape[1]:\n            loss = criterion(outputs.contiguous().view(-1, vocab_size),\n                             captions.contiguous().view(-1))\n        loss = criterion(outputs.contiguous().view(-1, vocab_size),captions.view(-1))\n        loss.backward()\n        optimizer.step()\n            \n        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n        print('\\r' + stats, end=\"\")\n    \n    with torch.no_grad():\n        for i_step in range(1,val_step+1):\n            encoder.eval()\n            decoder.eval()\n            images, captions,caplens = next(iter(valid_dataloader))\n            images = images.to(device)\n            captions = captions.to(device)\n#             caplens = caplens.to(device)\n\n            features = encoder(images)\n            outputs = decoder(features, captions,caplens)\n            if outputs.shape[1] != captions.shape[1]:\n                loss = criterion(outputs.contiguous().view(-1, vocab_size),\n                                 captions.contiguous().view(-1))\n            loss = criterion(outputs.contiguous().view(-1, vocab_size),captions.view(-1))\n            \n            print('/n')\n            stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, val_step, loss.item(), np.exp(loss.item()))\n            print('\\r' + stats, end=\"\")\n        \n    if epoch % save_every == 0:\n        torch.save(decoder.state_dict(), 'decoder-%d.pkl' % epoch)\n        torch.save(encoder.state_dict(), 'encoder-%d.pkl' % epoch)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"id":"y87lbrZ45FA_","outputId":"bc0f8311-b2bf-4ecc-c936-e5eb6f780931","execution":{"iopub.status.busy":"2023-01-04T03:28:05.860620Z","iopub.execute_input":"2023-01-04T03:28:05.860955Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch [1/1], Step [31/795], Loss: 6.5462, Perplexity: 696.61097","output_type":"stream"}]},{"cell_type":"markdown","source":"##Validation","metadata":{"id":"osDw4m_g9GCB"}},{"cell_type":"code","source":"encoder_file = 'encoder-1.pkl' \ndecoder_file = 'decoder-1.pkl'\n\n# TODO #3: Select appropriate values for the Python variables below.\nembed_size = 512\nhidden_size = 512\n\n# The size of the vocabulary.\nvocab_size = len(test_dataloader.dataset.vocab)\n\n# Initialize the encoder and decoder, and set each to inference mode.\nencoder1 = EncoderCNN(embed_size)\nencoder1.eval()\ndecoder1 = DecoderRNN(embed_size, hidden_size, vocab_size)\ndecoder1.eval()\n\n# Load the trained weights.\nencoder1.load_state_dict(torch.load(encoder_file))\ndecoder1.load_state_dict(torch.load(decoder_file))\n\n# Move models to GPU if CUDA is available.\nencoder1.to(device)\ndecoder1.to(device)\n\n","metadata":{"id":"KTv8wp6_xwM8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO #4: Complete the function.\ndef clean_sentence(output):\n    list_string = []\n    \n    for idx in output:\n        list_string.append(test_dataloader.dataset.vocab.idx2word[idx])\n    list_string = list_string[1:-1] # Discard  and  words\n    sentence = ' '.join(list_string) # Convert list of string to full string\n    sentence = sentence.capitalize()  # Capitalize the first letter of the first word\n    return sentence\n","metadata":{"id":"_LeGPWbA_m_Z","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prediction():\n    orig_image, image, caption = next(iter(test_dataloader))\n    plt.imshow(np.squeeze(orig_image))\n    plt.title('Sample Image')\n    plt.show()\n    image = image.to(device)\n    features = encoder1(image).unsqueeze(1)\n    output = decoder1.sample(features)    \n    sentence = clean_sentence(output)\n    print(f'Prediction: {sentence}'\n         f'Ground Truth: {caption}')\n\n","metadata":{"id":"3N3NgJDIAdt1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_prediction()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_prediction()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_prediction()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_prediction()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_prediction()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Do not Run","metadata":{"id":"WNajVwM5bi9y"}},{"cell_type":"markdown","source":"Further Implementation: Using a transformer(DISTILBERT TRANSFORMER) as a text encoder, for the predicted caption from the LSTM, then projecting both Image Encodings and Captions Encodings and comparing each other, using the loss function to optimize both Image Encoder,Caption Encoder, Text Encoder","metadata":{"id":"U5lT4Et2a4Zg"}},{"cell_type":"code","source":"!pip install transformers -q\nfrom transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n!pip install timm -q\nimport timm","metadata":{"id":"2wc7Bfrwc5wy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    debug = False\n    batch_size = 128\n    num_workers = 2\n    head_lr = 1e-3\n    image_encoder_lr = 1e-4\n    text_encoder_lr = 1e-5\n    weight_decay = 1e-3\n    patience = 1\n    factor = 0.8\n    epochs = 2\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model_name = 'resnet50'\n    image_embedding = 2048\n    text_encoder_model = \"distilbert-base-uncased\"\n    text_embedding = 768\n    text_tokenizer = \"distilbert-base-uncased\"\n    max_length = 200\n\n    pretrained = True # for both image encoder and text encoder\n    trainable = True # for both image encoder and text encoder\n    temperature = 1.0\n\n    # image size\n    size = 224\n\n    # for projection head; used for both image and text encoders\n    num_projection_layers = 1\n    projection_dim = 256 \n    dropout = 0.1\n","metadata":{"id":"mZJleeYI3iEx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AvgMeter:\n    def __init__(self, name=\"Metric\"):\n        self.name = name\n        self.reset()\n\n    def reset(self):\n        self.avg, self.sum, self.count = [0] * 3\n\n    def update(self, val, count=1):\n        self.count += count\n        self.sum += val * count\n        self.avg = self.sum / self.count\n\n    def __repr__(self):\n        text = f\"{self.name}: {self.avg:.4f}\"\n        return text\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group[\"lr\"]\n\n","metadata":{"id":"orYOJVhB3Bne","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        super(EncoderCNN, self).__init__()\n        resnet = torchvision.models.resnet50(pretrained='True')\n        for param in resnet.parameters():\n            param.requires_grad_(False)\n\n        modules = list(resnet.children())[:-1]\n        self.resnet = nn.Sequential(*modules)\n        self.embed = nn.Linear(resnet.fc.in_features,embed_size)\n\n  \n    def forward(self, images):\n        features = self.resnet(images)\n        features = features.view(features.size(0),-1)\n        features = self.embed(features)\n        return features","metadata":{"id":"htabHLipbaRp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageEncoder(nn.Module):\n    \"\"\"Encode Images to a fixed size vector\"\"\"\n    def __init__(self,model_name=CFG.model_name,pretrained=CFG.pretrained,trainable=CFG.trainable):\n        super().__init__()\n        self.model = timm.create_model(model_name,pretrained,num_classes=0,global_pool=\"avg\")\n        for p in self.model.parameters():\n            p.requires_grad = trainable\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"id":"86pHhhTMd945","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self,embed_size,hidden_size,vocab_size):\n        super(DecoderRNN,self).__init__()\n        self.lstm = nn.LSTM(input_size=embed_size,hidden_size=hidden_size, num_layers=1,\n                            bias=True,batch_first=True,dropout=0, bidirectional=False) \n    \n        self.embed = nn.Embedding(vocab_size,embed_size)\n        self.linear = nn.Linear(hidden_size,vocab_size)\n  \n    def init_hidden(self, batch_size):\n        return (torch.zeros((1,batch_size,hidden_size),device=device),\n                torch.zeros((1,batch_size,hidden_size),device=device))\n\n    def forward(self, features, captions):\n        captions = captions[:,:-1]\n        batch_size = features.shape[0] \n        hidden = self.init_hidden(batch_size)\n        embeddings = self.embed(captions)\n        embeddings = torch.cat((features.unsqueeze(1),embeddings),dim=1)\n        lstm_out, hidden = self.lstm(embeddings, hidden) \n        outputs = self.linear(lstm_out)\n        return outputs\n\n    def sample(self,inputs):\n        output = []\n        batch_size = inputs.shape[0] \n        hidden = self.init_hidden(batch_size)\n        while True:\n            lstm_out, hidden = self.lstm(inputs,hidden) \n            outputs = self.linear(lstm_out) \n            outputs = outputs.squeeze(1) \n            _, max_indice = torch.max(outputs,dim=1) \n            output.append(max_indice.cpu().numpy()[0].item())\n            if(max_indice == 1):\n                break\n            inputs = self.embed(max_indice) \n            inputs = inputs.unsqueeze(1) \n            \n        return output\n","metadata":{"id":"4dbdMHIMb9DO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextEncoder(nn.Module):\n    def __init__(self, model_name=\"distilbert-base-uncased\", pretrained=True, trainable=True):\n        super().__init__()\n        if pretrained:\n            self.model = DistilBertModel.from_pretrained(model_name)\n        else:\n            self.model = DistilBertModel(config=DistilBertConfig())\n    # for p in self.model.parameters():\n    #   p.requires_grad = trainable\n\n    #we are using the CLS token hidden representation as the sentence's embedding\n        self.target_token_idx = 0\n\n    def forward(self, input_ids, attention_mask):\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        last_hidden_state = output.last_hidden_state\n        return last_hidden_state[:,self.target_token_idx,:]","metadata":{"id":"JQlFRK-dEHdL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ProjectionHead(nn.Module):\n    def __init__(self,embedding_dim,projection_dim=256,dropout=0.1):\n        super().__init__()\n        self.projection = nn.Linear(embedding_dim,projection_dim)\n        self.gelu = nn.GELU()\n        self.fc = nn.Linear(projection_dim,projection_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(projection_dim)\n\n    def forward(self, x):\n        projected = self.projection(x)\n        x = self.gelu(projected)\n        x = self.fc(x)\n        x = self.dropout(x)\n        x = x+projected\n        x = self.layer_norm(x)\n        return x","metadata":{"id":"xNVf7_PHFP9L","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLIPModel(nn.Module):\n    def __init__(self,tokenizer,EncoderCNN,DecoderRNN,temperature=CFG.temperature,image_embedding=CFG.image_embedding,text_embedding=CFG.text_embedding):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.encodercnn = EncoderCNN\n        self.decoderrnn = DecoderRNN\n        self.image_encoder = ImageEncoder()\n        self.text_encoder = TextEncoder()\n        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n        self.temperature = temperature\n\n    def forward(self,images,captions):\n        #Getting Image and and Text Features\n        features = self.encodercnn(images)\n        outputs = self.decoderrnn(features, captions)\n        loss = criterion(outputs.contiguous().view(-1, vocab_size),captions.view(-1))\n        features2 = self.encodercnn(images).unsqueeze(1)\n        output2 = self.decoderrnn.sample(features2)    \n        sentence = clean_sentence(output2,train_dataloader)\n        z = self.tokenizer(sentence,padding=True,truncation=True,max_length=CFG.max_length)\n        image_features = self.image_encoder(images)\n        text_features = self.text_encoder(input_ids=z[\"input_ids\"],attention_mask=z[\"attention_mask\"])\n        #Getting Image and Text Embeddings (with same dimension)\n        image_embeddings = self.image_projection(image_features)\n        text_embeddings = self.text_projection(text_features)\n        #Calculating the Loss\n        logits = (text_embeddings @ image_embeddings.T)/self.temperature\n        images_similarity = image_embeddings @ image_embeddings.T\n        texts_similarity = text_embeddings @ text_embeddings.T\n        targets = torch.softmax((images_similarity+texts_similarity)/2*self.temperature,dim=-1)\n        texts_loss = cross_entropy(logits, targets, reduction='none')\n        images_loss = cross_entropy(logits.T,targets.T,reduction='none')\n        loss = (images_loss + texts_loss + loss) / 3.0\n        return loss.mean()","metadata":{"id":"qMY_zSJFfbM6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_entropy(preds, targets, reduction='none'):\n    log_softmax = nn.LogSoftmax(dim=-1)\n    loss = (-targets * log_softmax(preds)).sum(1)\n    if reduction =='none':return loss\n    elif reduction == \"mean\": return loss.mean()","metadata":{"id":"0jjAtoEKcpGO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO #4: Complete the function.\ndef clean_sentence(output,dataloader):\n    list_string = []\n    \n    for idx in output:\n        list_string.append(dataloader.dataset.vocab.idx2word[idx])\n    list_string = list_string[1:-1] # Discard  and  words\n    sentence = ' '.join(list_string) # Convert list of string to full string\n    sentence = sentence.capitalize()  # Capitalize the first letter of the first word\n    return sentence\n","metadata":{"id":"mwnNTbKxhzIR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(train_dataloader.dataset.vocab)\ncriterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\ntokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n\n# Initialize the encoder and decoder. \nembed_size = 512           \nhidden_size = 512  \nencoder = EncoderCNN(embed_size)\ndecoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n\n","metadata":{"id":"_cCIS5RjfaY3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model,train_loader,optimizer):\n    loss_meter = AvgMeter()\n    tqdm_object = tqdm(train_loader,total=len(train_loader))\n    for (images,captions) in tqdm_object:\n        images, captions = images.to(device),captions.to(device)\n        loss = model(images, captions)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        count = images.size(0)\n        loss_meter.update(loss.item(),count)\n        tqdm_object.set_postfix(train_loss=loss_meter.avg,lr=get_lr(optimizer))\n    return loss_meter\n\ndef valid_epoch(model,valid_loader):\n    loss_meter = AvgMeter()\n    tqdm_object = tqdm(valid_loader,total=len(valid_loader))\n    for (images, captions) in tqdm_object:\n        images, captions = images.to(device),captions.to(device)\n        loss = model(images, captions)\n        count = images.size(0)\n        loss_meter.update(loss.item(),count)\n        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n    return loss_meter\n\n","metadata":{"id":"m0n4Rvgf6ej9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CLIPModel(tokenizer,encoder,decoder).to(CFG.device)\nparams = [\n    {\"params\":model.encodercnn.embed.parameters(),\"lr\":CFG.image_encoder_lr},\n    {\"params\":model.decoderrnn.parameters(),\"lr\":0.001},\n    {\"params\":model.image_encoder.parameters(),\"lr\":CFG.image_encoder_lr},\n    {\"params\":model.text_encoder.parameters(),\"lr\":CFG.text_encoder_lr},\n    {\"params\":itertools.chain(model.image_projection.parameters(),model.text_projection.parameters()),\n      \"lr\":CFG.head_lr,\"weight_decay\":CFG.weight_decay}\n]\noptimizer = torch.optim.AdamW(params,weight_decay=0.)\n\nbest_loss = float('inf')\nfor epoch in range(CFG.epochs):\n    print(f\"Epoch: {epoch + 1}\")\n    model.train()\n    train_loss = train_epoch(model,train_dataloader,optimizer)\n    model.eval()\n\nwith torch.no_grad():\n    valid_loss = valid_epoch(model,test_dataloader)\n    if valid_loss.avg < best_loss:\n        best_loss = valid_loss.avg\n        torch.save(model.state_dict(),\"best.pth\")\n    print(\"Saved Best Model!\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["ed8ec78b91844518a60fbc040df4a708","818dd1c7918c40f2821f76d4b62d1107","cf4deeda93c2443caaf9a69eb1ea1623","49895386db4d494ebc32843aedb97b50","f4da2e3e7a97402085c50ed3098b3ed3","24632c50ed2f46a2a05eb51011f998bf","4fa7ab077d704df4b008cd67a4bff6d0","7686cef7548d4bdf936f42bf8002826a","ed57442cc5ca481daa3cc517d7f392c2","e6a583f6d36941d784c3d668512e2f6d","9d36fab3e81348ca90b38b81efa3029b"]},"id":"gjG-jtrUkEZl","outputId":"371dfd22-b79b-4cdf-c09a-2dd61ac5e8ff","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"z7PtqYP1oBBe"},"execution_count":null,"outputs":[]}]}